# -*- coding: utf-8 -*-
"""Basketball_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dvQCPvt16D5yAXbzXrkx8PpXSHSfz48J
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import pandas as pd
import tensorflow as tf
from matplotlib import pyplot as plt
import os
from google.colab import files
uploaded = files.upload()

from numpy import argmax
import re
import numpy as np

training_df = pd.read_csv("Seasons_Stats.csv")



# Print the first rows of the pandas DataFrame.
training_df.head()

training_df["PTS"] /= 1000.0   # Scale the label.

pd.options.display.max_rows = 10
pd.options.display.float_format = "{:.1f}".format

test_df = pd.read_csv("Seasons_Stats.csv")

test_df=test_df[test_df.PTS > 50]
test_df=test_df[test_df.Year >= 2016]   #Test for years since 2016
test_df.head()




training_df = pd.read_csv("Seasons_Stats.csv")
training_df=training_df[training_df.Year < 2016]
training_df=training_df[training_df.Year > 2012]

training_df.head()

training_df[training_df.PTS > 50]

del training_df['Year']
del training_df['Unnamed: 0']
del training_df['Player']
del training_df['Pos']
del training_df['Tm']
del training_df['Age']
del training_df['blanl']
del training_df['blank2']

del test_df['Year']
del test_df['Unnamed: 0']
del test_df['Player']
del test_df['Pos']
del test_df['Tm']
del test_df['Age']
del test_df['blanl']
del test_df['blank2']

#inefficient way to delete all non numeric variables from test and training data frames. 
#For now I don't need player names, but in the future
#I would use an integer enocding to learn more about player specific performance

# Normalize
train_df_mean = training_df.mean()
train_df_std = training_df.std()
train_df_norm = (training_df - train_df_mean)/train_df_std

test_df_mean = test_df.mean()
test_df_std = test_df.std()
test_df_norm = (test_df - test_df_mean)/test_df_std

test_df.head()
training_df.head()



# Create an empty list that will eventually hold all created feature columns.
feature_columns = []

resolution_in_Zs = 0.3  # 3/10 of a standard deviation.

# Create a bucket feature column for offensive win shares.

OWS_as_a_numeric_column = tf.feature_column.numeric_column("OWS")

OWS_boundaries = list(np.arange(int(min(train_df_norm['OWS'])), 
                                     int(max(train_df_norm['OWS'])), 
                                     resolution_in_Zs))

OWS = tf.feature_column.bucketized_column(OWS_as_a_numeric_column, OWS_boundaries)

#Create a bucket feature column for defensive win shares.


DWS_as_a_numeric_column = tf.feature_column.numeric_column("DWS")

DWS_boundaries = list(np.arange(int(min(train_df_norm['DWS'])), 
                                      int(max(train_df_norm['DWS'])), 
                                      resolution_in_Zs))

DWS = tf.feature_column.bucketized_column(DWS_as_a_numeric_column, 
                                                DWS_boundaries)

# Create a feature cross of OWS and DWS.
OWS_x_DWS = tf.feature_column.crossed_column([OWS,DWS],hash_bucket_size=30)
crossed_feature_1 = tf.feature_column.indicator_column(OWS_x_DWS)
feature_columns.append(crossed_feature_1)  

# Create a bucket feature column for offensive rebounds.

ORB_as_a_numeric_column = tf.feature_column.numeric_column("ORB")

ORB_boundaries = list(np.arange(int(min(train_df_norm['ORB'])), 
                                     int(max(train_df_norm['ORB'])), 
                                     resolution_in_Zs))

ORB = tf.feature_column.bucketized_column(ORB_as_a_numeric_column, ORB_boundaries)

#Create a bucket feature column for defensive rebounds.


DRB_as_a_numeric_column = tf.feature_column.numeric_column("DRB")

DRB_boundaries = list(np.arange(int(min(train_df_norm['DRB'])), 
                                      int(max(train_df_norm['DRB'])), 
                                      resolution_in_Zs))

DRB = tf.feature_column.bucketized_column(DRB_as_a_numeric_column, 
                                                DRB_boundaries)

# Create a feature cross of ORB and DRB.
ORB_x_DRB = tf.feature_column.crossed_column([ORB,DRB],hash_bucket_size=30)
crossed_feature_2 = tf.feature_column.indicator_column(ORB_x_DRB)
feature_columns.append(crossed_feature_2)  

# Represent Turnovers as a floating-point value.
Turnovers = tf.feature_column.numeric_column("TOV")
feature_columns.append(Turnovers)

# Represent Steals as a floating-point value.
Steals = tf.feature_column.numeric_column("STL")
feature_columns.append(Steals)

# Represent population as a floating-point value.
Three_Point_Attempts = tf.feature_column.numeric_column("3PA")
feature_columns.append(Three_Point_Attempts)

# Represent population as a floating-point value.
Minutes_Played = tf.feature_column.numeric_column("MP")
feature_columns.append(Minutes_Played)


# Represent population as a floating-point value.
Two_Point_Attempts = tf.feature_column.numeric_column("2PA")
feature_columns.append(Two_Point_Attempts)

# Represent Field Goal Attempts as a floating-point value.
Field_Goal_Attempts = tf.feature_column.numeric_column("FGA")
feature_columns.append(Field_Goal_Attempts)
 
my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

def plot_the_loss_curve(epochs, mse):
  """Plot a curve of loss vs. epoch."""

  plt.figure()
  plt.xlabel("Epoch")
  plt.ylabel("Mean Squared Error")

  plt.plot(epochs, mse, label="Loss")
  plt.legend()
  plt.ylim([mse.min()*0.95, mse.max() * 1.03])
  plt.show()  

print("Defined the plot_the_loss_curve function.")

def train_model(model, dataset, epochs, label_name,
                batch_size=None):

  # Split the dataset into features and label.
  features = {name:np.array(value) for name, value in dataset.items()}
  label = np.array(features.pop(label_name))
  history = model.fit(x=features, y=label, batch_size=batch_size,
                      epochs=epochs, shuffle=True) 

  #store epochs separately from the rest of history
  epochs = history.epoch
  
  # To track the progression of training, gather a snapshot
  # of the model's mean squared error at each epoch. 
  hist = pd.DataFrame(history.history)
  mse = hist["mean_squared_error"]

  return epochs, mse

def create_model(my_learning_rate, my_feature_layer):
  #Create and compile a simple linear model
  model = tf.keras.models.Sequential()

  model.add(my_feature_layer)

# Define the first hidden layer with 32 nodes. 
  model.add(tf.keras.layers.Dense(units=32, 
                                  activation='relu', 
                                  name='Hidden1'))

# Define the second hidden layer with 20 nodes. 
  model.add(tf.keras.layers.Dense(units=20, 
                                  activation='relu', 
                                  name='Hidden2'))
  
  # Define the third hidden layer with 10 nodes. 
  model.add(tf.keras.layers.Dense(units=10, 
                                  activation='relu', 
                                  name='Hidden3'))
  
  
  
  # Define the output layer.
  model.add(tf.keras.layers.Dense(units=1,  
                                  name='Output'))                              
  
  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),
                loss="mean_squared_error",
                metrics=[tf.keras.metrics.MeanSquaredError()])

  return model

# The following variables are the hyperparameters.
learning_rate = 0.01
epochs = 40
batch_size = 1000

# Specify the label
label_name = "PTS"

# Establish the model's topography.
my_model = create_model(learning_rate, my_feature_layer)

# Train the model on the normalized training set. We're passing the entire
# normalized training set, but the model will only use the features
# defined by the feature_layer.
epochs, mse = train_model(my_model, train_df_norm, epochs, 
                          label_name, batch_size)
plot_the_loss_curve(epochs, mse)

# After building a model against the training set, test that model
# against the test set.
test_features = {name:np.array(value) for name, value in test_df_norm.items()}
test_label = np.array(test_features.pop(label_name)) # isolate the label
print("\n Evaluate the new model against the test set:")
my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)